{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\RR}{\\mathbf{R}}\n",
    "\\newcommand{\\ZZ}{\\mathbf{Z}}\n",
    "\\newcommand{\\NN}{\\mathbf{N}}\n",
    "\\newcommand{\\e}{\\mathrm{e}}\n",
    "\\newcommand{\\eps}{\\varepsilon}\n",
    "$\n",
    "\n",
    "# HW 1\n",
    "\n",
    "- Submitted 2/3/18 by Colton Grainger for Math-428: Numerical Methods, Spring 2018, UIdaho Engineering Outreach.\n",
    "\n",
    "## Number Systems\n",
    "### prob 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to [radix](https://en.wikipedia.org/wiki/Radix) $10$: $(1011011.001)_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# represent (1011011.001)_2 as a \"float\"\n",
    "mantissa = [1,0,1,1,0,1,1,0,0,1]\n",
    "exponent = 7\n",
    "\n",
    "# initialize decimal rep\n",
    "c = 0\n",
    "\n",
    "# add powers of 2 to decimal rep\n",
    "for i in range(len(mantissa)):\n",
    "    c = c + mantissa[i]*(2^(-i-1))\n",
    "    \n",
    "# shift decimal point\n",
    "c = c*2^exponent\n",
    "\n",
    "print(str(c) + \" = \" + str(n(c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to radix $2$: $(2018)_{10}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer to be converted\n",
    "m = 2018\n",
    "\n",
    "# find the greatest i s.th. 2**i < m\n",
    "i = 0\n",
    "while m > 2**i:\n",
    "    i = i + 1\n",
    "i = i - 1\n",
    "\n",
    "# initialize empty list and counter\n",
    "c = []\n",
    "N = 0\n",
    "\n",
    "# remove powers of 2 from m, create binary rep\n",
    "while m != 0 and N < 15: # note stopping condition\n",
    "    if m >= 2**i:\n",
    "        m = m - 2**i\n",
    "        c.append(1)\n",
    "    else:\n",
    "        c.append(0)\n",
    "    i += -1\n",
    "    N += 1\n",
    "print(str(c)+\"_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prob 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The floating point representation of a real number is $x=\\pm\n",
    "(0.d_{1}d_{2}\\ldots d_{n})_\\beta\\cdot \\beta^e$, where\n",
    "$d_{1}\\not=0$, $-M\\leq e\\leq M$. Suppose that $\\beta=2$, $n=6$,\n",
    "$M=5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the smallest (positive) number in this floating point system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# represent (0.100000)_2 * 2^-5 as a \"float\"\n",
    "mantissa = [1,0,0,0,0,0]\n",
    "exponent = -5\n",
    "\n",
    "# create decimal rep\n",
    "c = 0\n",
    "for i in range(len(mantissa)):\n",
    "    c = c + mantissa[i]*(2^(-i-1))\n",
    "c = c*2^exponent\n",
    "\n",
    "print(str(c) + \" = \" + str(n(c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the largest number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# represent (0.111111)_2 * 2^5 as a \"float\"\n",
    "mantissa = [1,1,1,1,1,1]\n",
    "exponent = 5\n",
    "\n",
    "# create decimal rep\n",
    "c = 0\n",
    "for i in range(len(mantissa)):\n",
    "    c = c + mantissa[i]*(2^(-i-1))\n",
    "c = c*2^exponent\n",
    "\n",
    "print(str(c) + \" = \" + str(n(c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prob 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_binary(n): ## n must be a natural number\n",
    "    q = [n]\n",
    "    a = []\n",
    "    j = 0\n",
    "    while q[-1] != 0:\n",
    "        q.append(q[j] // 2)\n",
    "        a.append(q[j] % 2)\n",
    "        j = j + 1\n",
    "    print(str(list(reversed(a)))+'_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_binary(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_binary(56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_binary(1543)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite Precision Arithmetic\n",
    "\n",
    "### prob 3\n",
    "\n",
    "Near certain values of $x$ each of the following functions\n",
    "cannot be accurately  computed using the formula as given due to\n",
    "cancellation error. Identify the values of $x$ which are involved\n",
    "(e.g. near $x=0$ or large positive $x$) and propose a\n",
    "reformulation of the function (e.g., using Taylor series,\n",
    "rationalization, trigonometric identities, etc.) to remedy the\n",
    "problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example.** The function $f(x)=1+\\cos x$ cannot be computed accurately on the open balls $V_\\eps(x)$ with $x \\in \\{2k\\pi+\\pi: k \\in \\ZZ\\}$ for small $\\eps$. \n",
    "\n",
    "For example, given $\\eps = 0.5*10^{-7}$ on the open ball $V_\\eps(\\pi)$ we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 1 + cos(x)\n",
    "eps = 0.5*10^(-7)\n",
    "plot(f, (x,pi-eps, pi+eps), color = 'red').show(figsize = [7,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can, however, approximate $f$ about all $x \\in \\{2k\\pi+\\pi: k \\in \\ZZ\\}$ with a truncated 2nd order Taylor polynomial. Consider $x$ about $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(f.taylor(x, pi, 4))\n",
    "A = plot(f, (x,pi-eps, pi+eps), color = 'red')\n",
    "B = plot(f.taylor(x, pi, 2), (x,pi-eps, pi+eps), linestyle = '--')\n",
    "show(A+B,figsize = [7,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example.** Consider $f(x)=\\e^{-x}+\\sin x-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = e^(-x) + sin(x) - 1\n",
    "plot(f, (x, -1, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are looking for roots of multiplicity greater than $1$, as near these roots to compute $f$ we must subtract close numbers. \n",
    "\n",
    "Equivalently, we're looking for open balls $V_\\eps(x)$ where $f(x) = 0$ and $f'(x) \\approx 0$.\n",
    "\n",
    "- $V_\\eps(0)$ is such an open ball. \n",
    "- We don't care about the root near $2$, as $|f'(x)|_{x\\approx 2}$ is large.\n",
    "- There are roots near $7.10000000008731$ and $7.85000000006985$, but the slope of $f$ is large enough to prevent significant cancellation of digits. \n",
    "- For $x > 10$, the transient term $e^{-x}$ is negligible, and \"problematic\" open balls are thus $V_\\eps(x)$ with $x \\in \\{2k\\pi+\\pi/2 : k \\in \\NN, k > 2\\}$.\n",
    "\n",
    "Aside: On my hardware, what's the least $M$ such that $\\e^{-x}+\\sin x-1$ computes to a value less than or equal to $0$ for all $x > M$? I feel like I need to know the difference between [machine epsilon](https://en.wikipedia.org/wiki/Machine_epsilon) and the [unit in the last place](https://en.wikipedia.org/wiki/Unit_in_the_last_place)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import sign\n",
    "\n",
    "# to find roots, we bisect\n",
    "def bisect(f, a, b, eps, Nmax):\n",
    "    i=0\n",
    "    while b - a > eps and i < Nmax:\n",
    "        p = a + (b-a)/2\n",
    "        if sign(f(x=p)) == sign(f(x=b)):\n",
    "            b = p\n",
    "        else:\n",
    "            a = p\n",
    "        i += 1\n",
    "    return p\n",
    "\n",
    "# plot \"non-problematic\" root\n",
    "a = 7.1\n",
    "b = 7.85\n",
    "eps = 10^(-10)\n",
    "p = bisect(f, a, b, eps, 50)\n",
    "(point((p,0), size = '40') + plot(f, (x, p - eps, p + eps))).show(figsize=[7,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot another \"non-problematic\" root\n",
    "a = 7.85\n",
    "b = 8\n",
    "eps = 10^(-10)\n",
    "p = bisect(f, a, b, eps, 50)\n",
    "(point((p,0), size = '40') + plot(f, (x, p - eps, p + eps))).show(figsize=[7,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remaining open balls $V_\\eps(x)$ with $x \\in \\{2k\\pi+\\pi/2 : k \\in \\NN, k > 2\\}$, we'll approximate $f$ with a 2nd order Taylor polynomial (dashed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 10^(-7)\n",
    "for i in [2*(k+3)*pi + pi/2 for k in range(4)]:\n",
    "    A = plot(f, (x, i-eps, i+eps), color = 'red')\n",
    "    B = plot(f.taylor(x, i, 2), (x, i-eps, i+eps), linestyle = '--')\n",
    "    show(A+B,figsize=[10,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example.** Expressing $f(x)=\\ln x-\\ln(1/x) = \\ln x^2$ should avoid cancellation about $x = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example.** The function $f(x)=\\sqrt{x^2+1}-\\sqrt{x^2+4}$ will compute the difference of close numbers for all $x$ such that $|x| >> 1$. We can avoid cancellation of digits by expressing $f$ as \n",
    "$$ \n",
    "x \\mapsto \\frac{-3}{\\sqrt{x^2 + 1} + \\sqrt{x^2 + 4}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = sqrt(x^2 + 1) - sqrt(x^2 + 4)\n",
    "M = 10^8\n",
    "A = plot(f, (x, M, M+1), color = 'red')\n",
    "B = plot(lambda x: -3/(sqrt(x^2+1)+sqrt(x^2+4)), (x, M, M+1), linestyle = '--')\n",
    "show(A+B,figsize=[10,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example.** Refactor $f(x)=1-2\\sin^2x = \\cos 2x$ to avoid cancellation about $x = \\ldots \\frac\\pi4, \\frac{3\\pi}4, \\frac{5\\pi}4, \\frac{7\\pi}4 \\ldots$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 10^(-14.2)\n",
    "A = plot(lambda x: 1-2*(sin(x))^2, (x, pi/4 - eps, pi/4 + eps), color = 'red',thickness = 2)\n",
    "B = plot(lambda x: cos(2*x), (x, pi/4 - eps, pi/4 + eps), linestyle = '--')\n",
    "show(A+B, figsize=[8,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example.** Consider $f(x)=\\ln(x+\\sqrt{x^2+1})$ about $x = 0$. Here, we approximate $f$ as a linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ln(x+sqrt(x^2 + 1))\n",
    "eps = 10^(-15)\n",
    "A = plot(f, (x,0,eps), color = 'red')\n",
    "B = plot(f.taylor(x, 0, 1), (x,0,eps), linestyle = '--')\n",
    "show(A+B, figsize=[7,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prob 5\n",
    "\n",
    "To show that floating point arithmetic is not associative, we'll compute the sums $\\sum_{k=1}^6\\frac{1}{3^k}$ (forward) and $\\sum_{k=1}^6\\frac{1}{3^k}$ (backward) in a finite precision number system, say, rounding at each step into $3$ radix-$10$ digits.\n",
    "\n",
    "Actually not. Arithmetic with $3$ radix-$10$ digits is approximately arithmetic with $\\frac{\\ln 30}{\\ln 2} \\approx 5$ bits of precision, so if we use the python function `numerical_approx(x,n)` to round a (53 bit) float $x$ to $n$ bits, choosing $n = 5$ bits should do the trick. Of course, we have to define `tadd(x,y,n)` as the binary operation on $x$ and $y$ that adds and rounds to $n$ bits of precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tadd(x,y,n):\n",
    "    return numerical_approx(x+y,n)\n",
    "\n",
    "def forward(n):\n",
    "    x = 0\n",
    "    for k in [1..6]:\n",
    "        y = numerical_approx(1/3^k, n)\n",
    "        x = tadd(x,y,n)\n",
    "    return x\n",
    "\n",
    "def backward(n):\n",
    "    x = 0\n",
    "    for k in [1..6]:\n",
    "        y = numerical_approx(1/3^(7-k),n)\n",
    "        x = tadd(x,y,n)\n",
    "    return x\n",
    "\n",
    "print(forward(5), backward(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that adding forwards (large numbers first) produces a result less than that of than adding backwards (small numbers first). This is because a relatively small number may contribute no significant bits when added to a large number with finite precision arithmetic.\n",
    "\n",
    "Extending this train of thought, we compute the two sums again at varying bits of precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = list_plot([(n,forward(n)) for n in [2..20]], size = '25', color = 'red', legend_label = 'forward') \n",
    "B = list_plot([(n,backward(n)) for n in [2..20]], size = '50', legend_label = 'backward') \n",
    "show(B + A, axes_labels=['bits of precision', 'sum value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite Difference Approximation\n",
    "### prob 6\n",
    "\n",
    "Let $f(x)$ be a given function and recall the forward difference approximation of $f'(x)$\n",
    "$$\n",
    "D_+f(x)=\\frac{f(x+h)-f(x)}{h},\n",
    "$$\n",
    "where $h>0$ is the step size.\n",
    "\n",
    "Take $f(x)=\\sin x$, $x=\\pi/4$, $h=2^{-n}$ for $n=1,2,\\ldots,6$. \n",
    "\n",
    "We plot the error versus $h$ (use a log-log plot) and print the following information\n",
    "\n",
    "\n",
    "- $h$\n",
    "- $D_+f$\n",
    "- $f'(\\pi/4)-D_+f$\n",
    "- $(f'(\\pi/4)-D_+f)/h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var('x h')\n",
    "f(x) = sin(x)\n",
    "Dpf(x,h) = (f(x + h) - f(x))/h\n",
    "error(x,h) = f.diff(x)(x) - Dpf(x,h)\n",
    "\n",
    "x0 = pi/4\n",
    "A = list_plot([(h,error(x0,h)) for h in [2^(-n) for n in [1..6]]], scale = 'loglog')\n",
    "A.show(axes_labels=['mesh size $h$','absolute error $[f\\'-D_+f]_{x=\\\\frac{\\\\pi}{4}}$'],figsize=[7,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in [2^(-n) for n in [1..6]]:\n",
    "    print '      h = ' + str(numerical_approx(h))\n",
    "    print '    Dpf = ' + str(numerical_approx(Dpf(x0,h)))\n",
    "    print '  error = ' + str(numerical_approx(error(x0,h)))\n",
    "    print 'error/h = ' + str(numerical_approx(error(x0,h)/h))\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat for central difference approximation,\n",
    "$$\n",
    "D_0f(x)=\\frac{f(x+h)-f(x-h)}{2h},\n",
    "$$\n",
    "which also approximates $f'(x)$. \n",
    "\n",
    "The central difference approximation is more accurate; the order of error seems to be $O(h^2)$?\n",
    "\n",
    "TODO: firm up discussion about \"order of error\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var('x h')\n",
    "f(x) = sin(x)\n",
    "D0f(x,h) = (f(x + h) - f(x - h))/(2*h)\n",
    "error(x,h) = f.diff(x)(x) - D0f(x,h)\n",
    "\n",
    "x0 = pi/4\n",
    "A = list_plot([(h,error(x0,h)) for h in [2^(-n) for n in [1..6]]], scale = 'loglog')\n",
    "A.show(axes_labels=['mesh size $h$','absolute error $[f\\'-D_0f]_{x=\\\\frac{\\\\pi}{4}}$'],figsize=[7,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in [2^(-n) for n in [1..6]]:\n",
    "    print '      h = ' + str(numerical_approx(h))\n",
    "    print '    Dpf = ' + str(numerical_approx(D0f(x0,h)))\n",
    "    print '  error = ' + str(numerical_approx(error(x0,h)))\n",
    "    print 'error/h = ' + str(numerical_approx(error(x0,h)/h))\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prob 7: central concavity approx\n",
    "\n",
    "The forward and backward finite-difference operators are defined by\n",
    "- $D_+f(x)=\\frac{f(x+h)-f(x)}{h}$\n",
    "- $D_-f(x)=\\frac{f(x)-f(x-h)}{h}$\n",
    "\n",
    "Composition (in either order) yields\n",
    "$$\n",
    "\\begin{align}\n",
    "    (D_+D_-f)(x) &= (D_+(D_-f))(x)\\\\\n",
    "        &= \\frac{(D_-f)(x+h) - (D_-f)(x)}h\\\\\n",
    "        &= \\frac{\\frac{f(x+h)-f(x)}h - \\frac{f(x)-f(x-h)}h}h\\\\\n",
    "        &= \\frac{f(x+h)-2f(x)+f(x-h)}{h^2}.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding $f(x+h)$ and $f(x-h)$ into $3$rd order Taylor polynomials ($4$th order remainders) we have \n",
    "\n",
    "- $f(x+h) = f(x) + hf^{(1)}(x) + \\frac{h^2}{2!} f^{(2)}(x) + \\frac{h^3}{3!} f^{(3)}(x) + \\frac{h^4}{4!}f^{(4)}(\\xi_1)$ with $\\xi_1 \\in (x, x+h)$.\n",
    "- $f(x-h) = f(x) - hf^{(1)}(x) + \\frac{h^2}{2!} f^{(2)}(x) - \\frac{h^3}{3!} f^{(3)}(x) + \\frac{h^4}{4!}f^{(4)}(\\xi_2)$ with $\\xi_2 \\in (x-h, x)$.\n",
    "    \n",
    "Adding the polynomials above, substracting $2f(x)$, and dividing out $h^2$ we have\n",
    "\n",
    "$$\n",
    "\\frac{f(x+h)-2f(x)+f(x-h)}{h^2} = f^{(2)}(x) + \\frac{h^2}{4!}(f^{(4)}(\\xi_1) + f^{(4)}(\\xi_2)).\n",
    "$$\n",
    "\n",
    "Whence\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left|D_+D_-f(x) - f^{(2)}(x)\\right| \n",
    "    &= \\left|\\frac{h^2}{4!}(f^{(4)}(\\xi_1) + f^{(4)}(\\xi_2))\\right|\\\\\n",
    "    &\\leq \\frac{h^2}{12}\\left|f^{(4)}(\\xi)\\right| \\quad\\text{for some $\\xi \\in (x-h, x+h)$}\\\\\n",
    "    &\\leq \\frac{h^2}{12}\\cdot M \\quad\\text{for $M=\\max \\left|f^{(4)}(\\xi)\\right|$}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In summary, we've shown that $D_+D_-f(x)=f''(x)+O(h^2)$ with asymptotic error constant $\\frac{M}{12}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rootfinding\n",
    "### prob 8\n",
    "\n",
    "Consider $f(x) = x^2 - 5$. Since $f(2) < 0$, $f(3) > 0$, \n",
    "it follows that $f(x)$ has a root $p$ in the\n",
    "interval $(2, 3)$. Compute an approximation to $p$ \n",
    "by the following methods. Take $10$ steps in\n",
    "each case. Print the answers to $15$ digits.\n",
    "\n",
    "Print the following information for each method. Do the results agree\n",
    "with the theory discussed in class?\n",
    "- $n$ (step)\n",
    "- $x_n$ (approximation)\n",
    "- $f(x_n)$  (residual)\n",
    "- $|p - x_n|$  (error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bisection method, starting interval $[a, b] = [2, 3]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from numpy import sign\n",
    "\n",
    "def bisect(f, root, a, b, eps, Nmax, PRINT = True):\n",
    "    i=0\n",
    "    while b - a > eps and i < Nmax:\n",
    "        p = a + (b-a)/2\n",
    "        if sign(f(x=p)) == sign(f(x=b)):\n",
    "            b = p\n",
    "        else:\n",
    "            a = p\n",
    "        i += 1\n",
    "        if PRINT:\n",
    "            print \"    step \"+str(i)\n",
    "            print \"  approx \"+str(numerical_approx(p))\n",
    "            print \"residual \"+str(numerical_approx(f(x=p)))\n",
    "            print \"   error \"+str(numerical_approx(root-p))\n",
    "            print\n",
    "    return numerical_approx(p)\n",
    "\n",
    "f = x^2 - 5\n",
    "root=sqrt(5); a=2; b=3; eps=10^(-15); Nmax=10\n",
    "bisect(f, root, a, b, eps, Nmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fixed-point iteration with $g_1(x) = 5/x$ and $g_2(x) = x - f(x)/3$, starting value $2.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixedpt(f, g, root, p1, eps, Nmax, PRINT = True):\n",
    "    i=0; p0=float(\"inf\")\n",
    "    while abs(p1 - p0) > eps and i < Nmax:\n",
    "        p0 = p1\n",
    "        p1 = g(x=p1)\n",
    "        i += 1\n",
    "        if PRINT:\n",
    "            print \"    step \"+str(i)\n",
    "            print \"  approx \"+str(p1)\n",
    "            print \"residual \"+str(f(x=p1))\n",
    "            print \"   error \"+str(numerical_approx(root-p1))\n",
    "            print\n",
    "    return numerical_approx(p1)\n",
    "\n",
    "f = x^2 - 5\n",
    "g1 = 5/x\n",
    "root=sqrt(5); p1=2.5; eps=10^(-15); Nmax=10\n",
    "fixedpt(f, g1, root, p1, eps, Nmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = x - f(x)/3\n",
    "root=sqrt(5); p1=2.5; eps=10^(-15); Nmax=10\n",
    "fixedpt(f, g2, root, p1, eps, Nmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newton's method, starting value $x_0 = 2.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = x - f(x)/(diff(f,x)(x))\n",
    "root = sqrt(5)\n",
    "root=sqrt(5); p1=2.5; eps=10^(-15); Nmax=10\n",
    "fixedpt(f, g, root, p1, eps, Nmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prob 9 \n",
    "\n",
    "Consider the function $g(x)=2x(1-x)$.\n",
    "\n",
    "We verify $x=0$ and $x=1/2$ are fixed points of $g(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 2*x*(1-x)\n",
    "print(g(x=0), g(x=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect that fixed point iteration, starting even with a value very close to zero, will fail to converge toward $x=0$ because (the following are equivalent)\n",
    "- $\\frac d{dx}\\left[g(x) - x\\right]_{x=0} > 0$\n",
    "- for all $x_{n} \\neq 0$ in sufficiently small open sets $U$ containing $0$, the fixed point iterations \"push away\", i.e., $|x_{n+1}|=|g(x_{n})| > |x_{n}|$\n",
    "- $0$ is an unstable equilibrium for some first order separable differential equation in $x$ whose phase space is represented by $g(x) - x$ (e.g., the logistic equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = plot(g, (x, -0.1, 1.1))\n",
    "B = plot(x, (x, -0.1, 1.1), linestyle = '--')\n",
    "show(A + B, ymax = 0.7, figsize = [7,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should expect that fixed point iteration starting with $p_0\\in(0,1)$ will converge toward $x=1/2$ because the image of $(0,1)$ under the mapping $g$ is $\\left(0, \\frac12\\right)$. In this case, it can be shown any sequence of iterations with terms in $\\left(0, \\frac12\\right)$ will converge to $\\frac12$. We should expect a linear order of convergence (see the log-log plot below---pretty linear)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naivefixedpt(g,p1,eps,Nmax,PRINT=True):\n",
    "    i=0; p0=float(\"inf\"); seq=[]\n",
    "    while abs(p1 - p0) > eps and i < Nmax:\n",
    "        p0 = p1\n",
    "        p1 = g(x=p1)\n",
    "        i += 1\n",
    "        seq.append(numerical_approx(p1))\n",
    "    return seq\n",
    "\n",
    "g = 2*x*(1-x)\n",
    "s = naivefixedpt(g, 0.99, 10^(-20),10)\n",
    "A = list_plot([(abs(s[n]-0.5),abs(s[n+1]-0.5)) for n in range(8)], size = '20')\n",
    "A.show(scale='loglog', axes_labels=['$|e_n|$','$|e_{n+1}|$'], figsize = [7,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To numerically confirm the order of convergence, we'll use linear regression to find the relation between $|e_{n+1}|$ and $|e_n|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = [(log(abs(s[n]-0.5)),log(abs(s[n+1]-0.5))) for n in range(8)]\n",
    "\n",
    "var('a b')\n",
    "model(x) = a*x+b\n",
    "find_fit(R,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: determine the asymptotic error constant from the linear regression (is this possible?)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 8.0",
   "language": "",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
